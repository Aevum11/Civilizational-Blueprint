#!/usr/bin/env python3
"""
Exception Theory Web Tracer and Downloader v2.1
The Complete ET-Derived Web Manifold Navigator with GUI and Logging

FIXES BASED ON VERIFICATION:
- Changed 'manifold_hash' to 'content_address' (exists in dir(ETMathV2))
- Changed 'cardinality' to 'set_cardinality_d' (exists in dir(ETMathV2))
- Changed 'recursive_descriptor_discovery' to 'descriptor_discovery_recursion' (exists in dir(ETMathV2Descriptor))
- Implemented simple variance_checker using ET-derived logic (diff of hashes / BASE_VARIANCE)
- Verified all method calls with hasattr checks
- Added more logging for method availability
- Fallbacks if methods still missing after verification

This resolves the attribute errors.

All generated files in script dir by default.

Author: Derived from Michael James Muller's Exception Theory
Version: 2.1 (2026-02-03)
"""

import os
import sys
import hashlib
import subprocess
import webbrowser
import logging
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set, Optional, Any

def main():
    # Get script directory for all file operations
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Setup Logging in Script Directory
    log_path = os.path.join(script_dir, 'et_downloader.log')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info(f"Script started. Log file: {log_path}")
    logger.info(f"Script directory: {script_dir}")
    logger.info(f"Current working directory: {os.getcwd()}")
    logger.info(f"sys.path: {sys.path}")

    # List contents of script_dir for troubleshooting
    try:
        dir_contents = os.listdir(script_dir)
        logger.info(f"Contents of script_dir: {dir_contents}")
    except Exception as e:
        logger.error(f"Failed to list script_dir: {str(e)}")

    # GUI Import (Standard Library)
    try:
        import tkinter as tk
        from tkinter import filedialog, messagebox
        logger.info("Tkinter imported successfully.")
    except ImportError as e:
        logger.error(f"Tkinter import failed: {str(e)}")
        messagebox.showerror("Import Error", f"Tkinter not found: {str(e)}")
        return

    # Function to check and install missing modules
    def install_missing_modules(modules: List[str]):
        """
        Verify and install needed components using pip.
        Derived from ET: Recursive discovery of missing descriptors (modules) and substantiation (installation).
        """
        for module in modules:
            try:
                __import__(module)
                logger.info(f"Module {module} already installed.")
            except ImportError:
                logger.warning(f"Module {module} missing. Attempting installation.")
                try:
                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', module])
                    logger.info(f"Successfully installed {module}.")
                except Exception as e:
                    logger.error(f"Failed to install {module}: {str(e)}")
                    messagebox.showerror("Installation Error", f"Failed to install {module}: {str(e)}\nPlease install manually.")

    # Install required external libraries if missing
    required_modules = ['requests', 'beautifulsoup4']
    install_missing_modules(required_modules)

    # Now safe to import
    try:
        import requests
        from bs4 import BeautifulSoup
        logger.info("requests and bs4 imported successfully.")
    except ImportError as e:
        logger.error(f"Import failed after installation attempt: {str(e)}")
        return

    # Adjust sys.path to include local exception_theory
    sys.path.insert(0, script_dir)
    logger.info(f"Added {script_dir} to sys.path for local imports.")
    logger.info(f"Updated sys.path: {sys.path}")

    # Import ET Core Components (Required for ET-Derived Math)
    try:
        from exception_theory.core.mathematics import ETMathV2
        from exception_theory.core.mathematics_descriptor import ETMathV2Descriptor
        from exception_theory.core.primitives import Point, Descriptor, Traverser, bind_pdt
        from exception_theory.core.constants import BASE_VARIANCE, MANIFOLD_SYMMETRY
        logger.info("Successfully imported exception_theory components.")
    except ImportError as e:
        logger.error(f"Failed to import exception_theory: {str(e)}")
        # Check for setup.txt and copy to setup.py if needed
        setup_txt = os.path.join(script_dir, 'setup.txt')
        setup_py = os.path.join(script_dir, 'setup.py')
        if os.path.exists(setup_txt) and not os.path.exists(setup_py):
            try:
                shutil.copy(setup_txt, setup_py)
                logger.info("Copied setup.txt to setup.py for installation.")
            except Exception as copy_e:
                logger.error(f"Failed to copy setup.txt to setup.py: {str(copy_e)}")
        
        if os.path.exists(setup_py):
            try:
                subprocess.check_call([sys.executable, setup_py, 'install'], cwd=script_dir)
                logger.info("Installed via setup.py. Retrying import.")
                from exception_theory.core.mathematics import ETMathV2
                from exception_theory.core.mathematics_descriptor import ETMathV2Descriptor
                from exception_theory.core.primitives import Point, Descriptor, Traverser, bind_pdt
                from exception_theory.core.constants import BASE_VARIANCE, MANIFOLD_SYMMETRY
                logger.info("Retry import successful.")
            except Exception as install_e:
                logger.error(f"Installation via setup.py failed: {str(install_e)}")
                messagebox.showerror("ET Library Error", "Failed to import or install exception_theory. Check logs for details.")
            finally:
                if os.path.exists(setup_py) and os.path.exists(setup_txt):
                    try:
                        os.remove(setup_py)
                        logger.info("Cleaned up temporary setup.py.")
                    except:
                        logger.warning("Failed to clean up setup.py.")
        else:
            logger.error("No setup.py or setup.txt found.")
            messagebox.showerror("ET Library Error", "No setup file found. Ensure proper placement.")
        return

    # Instantiate ET Math classes
    try:
        et_math = ETMathV2()
        et_desc_math = ETMathV2Descriptor()
        logger.info("Instantiated ETMathV2 and ETMathV2Descriptor.")
        logger.info(f"dir(et_math): {dir(et_math)}")
        logger.info(f"dir(et_desc_math): {dir(et_desc_math)}")
    except Exception as inst_e:
        logger.error(f"Instantiation failed: {str(inst_e)}")
        return

    # Check for required methods and fallback if missing
    if not hasattr(et_math, 'content_address'):
        logger.error("content_address missing from ETMathV2. Using fallback.")
        def content_address(data: bytes) -> str:
            return hashlib.sha256(data).hexdigest()
    else:
        content_address = et_math.content_address

    if not hasattr(et_math, 'set_cardinality_d'):
        logger.error("set_cardinality_d missing from ETMathV2. Using fallback.")
        def set_cardinality_d(n: int) -> int:
            return n  # Simple finite
    else:
        set_cardinality_d = et_math.set_cardinality_d

    if not hasattr(et_desc_math, 'descriptor_discovery_recursion'):
        logger.error("descriptor_discovery_recursion missing from ETMathV2Descriptor. Using fallback.")
        def descriptor_discovery_recursion(data: str) -> List[Dict]:
            soup = BeautifulSoup(data, 'html.parser')
            descs = []
            for elem in soup.find_all(['a', 'img', 'link', 'script']):
                attr = 'href' if elem.name in ('a', 'link') else 'src'
                if elem.has_attr(attr):
                    descs.append({attr: elem[attr]})
            return descs
    else:
        descriptor_discovery_recursion = et_desc_math.descriptor_discovery_recursion

    # Define variance checker (not in library, so derive)
    def derive_integrity_variance_checker() -> callable:
        """
        Derive ET Math for content integrity (Eq 3: Variance Measurement).
        Script: Production-ready variance calculator for downloaded content.
        """
        def checker(original_hash: str, downloaded_data: bytes) -> float:
            computed = content_address(downloaded_data)
            if computed == original_hash:
                return 0.0
            else:
                # Simple diff as variance
                diff = sum(a != b for a, b in zip(original_hash, computed)) / len(original_hash)
                variance = diff / BASE_VARIANCE
                return variance if variance <= 1.0 else float('inf')
        
        return checker

    integrity_variance_checker = derive_integrity_variance_checker()

    # Script 1: Derive Web Resource Cardinality Estimator
    def derive_resource_cardinality_estimator() -> callable:
        """
        Derive ET Math for estimating resource cardinality (Eq 209: Descriptor Cardinality).
        Script: Production-ready function to calculate finite ways to describe web resources.
        """
        def estimator(resources: List[str]) -> int:
            # ET Derivation: |D| = n (finite), use power set cardinality bounded by manifold symmetry
            base_card = set_cardinality_d(len(resources))  # ET-derived cardinality
            bounded = int(base_card % MANIFOLD_SYMMETRY) or MANIFOLD_SYMMETRY  # Bind to symmetry
            return bounded * len(resources)  # Finite descriptions
        
        return estimator

    # Script 3: Derive Recursive Link Discoverer
    def derive_recursive_link_discoverer() -> callable:
        """
        Derive ET Math for link discovery (Eq 217: Recursive Descriptor Discovery).
        Script: Production-ready recursive function using ET descriptor discovery.
        """
        def discoverer(html_content: str, base_url: str, depth: int = 1) -> Set[str]:
            if depth <= 0:
                return set()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            links = set()
            
            # ET Derivation: Use descriptor_discovery_recursion to find links
            descriptors = descriptor_discovery_recursion(html_content)
            for desc in descriptors:
                if isinstance(desc, dict) and 'href' in desc or 'src' in desc:
                    url = desc.get('href') or desc.get('src')
                    full_url = urljoin(base_url, url)
                    links.add(full_url)
            
            # Recurse on discovered descriptors (bind more if needed)
            sub_links = set()
            for link in links:
                try:
                    sub_content = requests.get(link, timeout=5).text
                    sub_links.update(discoverer(sub_content, link, depth-1))
                except Exception as e:
                    logger.warning(f"Sub-traversal failed for {link}: {str(e)}")
            
            return links | sub_links
        
        return discoverer

    # Initialize Derived Functions
    resource_cardinality_estimator = derive_resource_cardinality_estimator()
    recursive_link_discoverer = derive_recursive_link_discoverer()
    logger.info("Derived ET math functions initialized.")

    class ETWebTraverser(Traverser):
        """
        ET-Derived Web Traverser Class
        Extends base Traverser to navigate web manifold.
        T: Agency crawling URLs (P) with content descriptors (D).
        """
        def __init__(self, identity: str, starting_url: str):
            super().__init__(identity=identity, current_point=Point(location=starting_url))
            self.downloaded: Dict[str, bytes] = {}  # Bound content (P ∘ D)
            self.hashes: Dict[str, str] = {}  # Descriptor hashes
            self.variances: Dict[str, float] = {}  # Integrity variances
            logger.info(f"Initialized ETWebTraverser for {starting_url}")
        
        def traverse_and_substantiate(self, url: str) -> Optional[bytes]:
            """
            Traverse to URL (P) and substantiate content (T ∘ D).
            Uses ET binding: bind_pdt(url_point, content_desc, self)
            """
            try:
                logger.info(f"Traversing to {url}")
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                content = response.content
                
                # ET Binding: Create Point, Descriptor, Bind with Traverser
                url_point = Point(location=url)
                content_desc = Descriptor(name="web_content", constraint=len(content))
                exception = bind_pdt(url_point, content_desc, self)
                
                # Hash and Verify (ET-derived math)
                content_hash = content_address(content)
                self.hashes[url] = content_hash
                
                # Calculate Variance (Derived integrity check)
                variance = integrity_variance_checker(content_hash, content)
                self.variances[url] = variance
                logger.info(f"Variance for {url}: {variance}")
                
                if variance > BASE_VARIANCE:  # ET threshold for acceptance
                    raise ValueError(f"High variance ({variance}) at {url} - unbound descriptor")
                
                self.downloaded[url] = content
                return content
            
            except Exception as e:
                logger.error(f"Traversal error at {url}: {str(e)}")
                return None
        
        def discover_resources(self, html_content: str, base_url: str) -> Set[str]:
            """
            Discover embedded resources using ET-derived recursive discovery.
            """
            try:
                logger.info(f"Discovering resources from {base_url}")
                return recursive_link_discoverer(html_content, base_url)
            except Exception as e:
                logger.error(f"Resource discovery failed: {str(e)}")
                return set()
        
        def download_all(self, base_url: str, output_dir: str) -> Dict[str, Any]:
            """
            Download page and all within (full trace).
            Uses resource_cardinality_estimator for bounding.
            """
            try:
                os.makedirs(output_dir, exist_ok=True)
                logger.info(f"Created output directory: {output_dir}")
                
                # Substantiate main page
                html_content = self.traverse_and_substantiate(base_url)
                if not html_content:
                    raise RuntimeError(f"Failed to substantiate base URL: {base_url}")
                
                # Save main HTML
                main_path = os.path.join(output_dir, 'index.html')
                with open(main_path, 'wb') as f:
                    f.write(html_content)
                logger.info(f"Saved main HTML to {main_path}")
                
                # Discover and download resources
                resources = self.discover_resources(html_content.decode('utf-8', errors='ignore'), base_url)
                logger.info(f"Discovered {len(resources)} resources")
                
                # ET Cardinality Check: Bound finite resources
                estimated_card = resource_cardinality_estimator(list(resources))
                if len(resources) > estimated_card:
                    logger.warning(f"Resource cardinality exceeds ET bound ({len(resources)} > {estimated_card})")
                
                for res_url in resources:
                    content = self.traverse_and_substantiate(res_url)
                    if content:
                        parsed = urlparse(res_url)
                        res_path = os.path.join(output_dir, parsed.path.lstrip('/')) or 'resource'
                        os.makedirs(os.path.dirname(res_path), exist_ok=True)
                        with open(res_path, 'wb') as f:
                            f.write(content)
                        logger.info(f"Saved resource to {res_path}")
                
                # Report (Exhaustive)
                report = {
                    'base_url': base_url,
                    'total_resources': len(resources),
                    'downloaded_count': len(self.downloaded),
                    'variances': self.variances,
                    'hashes': self.hashes,
                    'estimated_cardinality': estimated_card,
                    'manifold_symmetry': MANIFOLD_SYMMETRY  # ET constant used
                }
                logger.info(f"Download report: {report}")
                return report, main_path
            except Exception as e:
                logger.error(f"Download all failed: {str(e)}")
                raise

    def run_gui():
        """
        Basic GUI for input: URL and output folder.
        After download, open local index.html in default browser.
        """
        root = tk.Tk()
        root.withdraw()  # Hide main window
        
        # Get URL
        url = tk.simpledialog.askstring("Input URL", "Enter the webpage URL:")
        if not url:
            logger.error("URL input cancelled.")
            messagebox.showerror("Error", "URL is required.")
            return
        logger.info(f"User entered URL: {url}")
        
        # Get output folder, default to script_dir
        output_dir = filedialog.askdirectory(title="Select Output Folder", initialdir=script_dir)
        if not output_dir:
            logger.error("Output folder selection cancelled.")
            messagebox.showerror("Error", "Output folder is required.")
            return
        logger.info(f"User selected output dir: {output_dir}")
        
        # Create ET Traverser
        traverser = ETWebTraverser(identity="web_crawler", starting_url=url)
        
        try:
            report, local_path = traverser.download_all(url, output_dir)
            messagebox.showinfo("Success", f"Download Complete.\nTotal Resources: {report['total_resources']}\nDownloaded: {report['downloaded_count']}\nOpening in browser...")
            logger.info("Download successful. Opening browser.")
            
            # Open local index.html in default browser
            webbrowser.open(f"file://{os.path.abspath(local_path)}")
            
        except Exception as e:
            logger.error(f"Critical Error in download: {str(e)}")
            messagebox.showerror("Error", f"Critical Error: {str(e)}")

    run_gui()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Critical error: {str(e)}")  # Direct print for early errors
    finally:
        print("Program completed. Press Enter to exit.")
        input()  # Always wait for input, even if early crash